{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference video with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_CHECKPOINT = \"theButcher22/deformable-detr-football-finetuned\"\n",
    "MODEL_CHECKPOINT = \"theButcher22/deta-swin-large\"\n",
    "#MODEL_CHECKPOINT = \"../data/models/deta-swin/checkpoint_7450/\"\n",
    "PATH = \"../data/input_videos\"\n",
    "VIDEO_NAME = \"esa_04.mp4\"\n",
    "OUTPUT_PATH = \"../data/output_videos\"\n",
    "OUTPUT_VIDEO_NAME = \"esa_04_deta.mp4\"\n",
    "#VIDEO_PATH = 'esa_01.mov'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH\n",
    "\n",
    "import os\n",
    "\n",
    "video_files = [\n",
    "    (\"football_02.mp4\", \"https://drive.google.com/file/d/1t6agoqggZKx6thamUuPAIdN_1zR9v9S\"),\n",
    "    (\"football_03.mp4\", \"https://drive.google.com/uc?id=12TqauVZ9tLAv8kWxTTBFWtgt2hNQ4_ZF\"),\n",
    "    (\"football_04.mp4\", \"https://drive.google.com/uc?id=19PGw55V8aA6GZu5-Aac5_9mCy3fNxmEf\"),\n",
    "    (\"football_05.mp4\", \"https://drive.google.com/uc?id=1OG8K6wqUw9t7lp9ms1M48DxRhwTYciK-\"),\n",
    "    (\"football_06.mp4\", \"https://drive.google.com/uc?id=1yYPKuXbHsCxqjA9G-S6aeR2Kcnos8RPU\"),\n",
    "    (\"football_07.mp4\", \"https://drive.google.com/uc?id=1vVwjW1dE1drIdd4ZSILfbCGPD4weoNiu\"),\n",
    "]\n",
    "\n",
    "for filename, url in video_files:\n",
    "    file_path = os.path.join(path, filename)\n",
    "    print(file_path)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {filename} from {url}\")\n",
    "        !gdown -O \"{file_path}\" \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferance on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token=userdata.get('HF_TOKEN')\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(dotenv_path='../config/.env')\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
    "model = AutoModelForObjectDetection.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "video_path = os.path.join(path, VIDEO_NAME)\n",
    "\n",
    "id2label = {1: \"ball\", 2: \"goalkeeper\", 3: \"player\", 4: \"referee\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_inference(frame, threshold=0.5):\n",
    "    \"\"\"\n",
    "    This function performs inference on a single video frame to detect objects.\n",
    "\n",
    "    Parameters:\n",
    "    frame (numpy.ndarray): The input video frame as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "    detections (sv.Detections): An object containing the detected objects and their properties.\n",
    "    \n",
    "    The function processes the input frame using a pre-trained object detection model and returns the detections.\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "    img = Image.fromarray(frame)\n",
    "\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    width, height = img.size\n",
    "    target_size = torch.tensor([[height, width]])\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, threshold=threshold, target_sizes=target_size)[0]\n",
    "        \n",
    "    detections = sv.Detections.from_transformers(\n",
    "        transformers_results=results,\n",
    "        id2label=model.config.id2label\n",
    "    )\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame(frame, detections, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Annotates a given frame with detected objects.\n",
    "\n",
    "    Parameters:\n",
    "    frame (numpy.ndarray): The input frame to be annotated.\n",
    "    detections (sv.Detections): The detections obtained from the model.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The annotated frame with visual indicators for detected objects.\n",
    "    \n",
    "    The function uses different annotators to draw indicators for different objects in the frame. It specifically highlights the 'ball' \n",
    "    detection with a triangle.\n",
    "    \"\"\"\n",
    "    ellipse_annotator = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    "    )\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "        text_color=sv.Color.from_hex('#000000'),\n",
    "        text_position=sv.Position.BOTTOM_CENTER,\n",
    "        text_scale=0.5,\n",
    "        text_thickness=1,\n",
    "        text_padding=10,\n",
    "        smart_position=True\n",
    "    )\n",
    "    triangle_annotator = sv.TriangleAnnotator(\n",
    "    color=sv.Color.from_hex('#FFD700'),\n",
    "    base=25,\n",
    "    height=21,\n",
    "    outline_thickness=1\n",
    "    )\n",
    "    \n",
    "    labels = [\n",
    "        model.config.id2label[class_id]\n",
    "        for class_id\n",
    "        in detections.class_id\n",
    "    ]\n",
    "\n",
    "    ball_id = label2id[\"ball\"]  \n",
    "    ball_detections = detections[detections.class_id == ball_id]\n",
    "    ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "    all_detections = detections[detections.class_id != ball_id]\n",
    "    all_detections = all_detections.with_nms(threshold=threshold, class_agnostic=True)\n",
    "    all_detections.class_id -= 1\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    annotated_frame = ellipse_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections)\n",
    "    \n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections,\n",
    "        labels=labels)\n",
    "    \n",
    "    annotated_frame = triangle_annotator.annotate(\n",
    "    scene=annotated_frame,\n",
    "    detections=ball_detections)\n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot firt video frame\n",
    "frame_generator = sv.get_video_frames_generator(video_path)\n",
    "for _ in range(5):  # Skip the first x frames\n",
    "    next(frame_generator)\n",
    "frame = next(frame_generator)\n",
    "detections = frame_inference(frame, threshold=0.2)\n",
    "annotated_frame = annotate_frame(frame, detections, threshold=0.2)\n",
    "print(detections)\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "def inference_video(video_path: str, output_video_path: str,threshold=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    This function performs video inference using a specified video path and outputs the annotated video to a given output path.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path (str): The path to the input video file.\n",
    "    - output_video_path (str): The path where the output video will be saved.\n",
    "    - verbose (bool): A flag indicating whether to print detailed processing information for each frame. Default is False.\n",
    "\n",
    "    The function utilizes a tracker (ByteTrack) to maintain object identities across frames and a smoother to enhance detection stability.\n",
    "    It processes each frame of the video, applies detections, annotates the frames accordingly, and saves the annotated video to the specified output path.\n",
    "    \"\"\"\n",
    "    tracker = sv.ByteTrack()\n",
    "    smoother = sv.DetectionsSmoother()\n",
    "    if verbose:\n",
    "        frame_idx = 0\n",
    "        video_info = sv.VideoInfo.from_video_path(video_path)\n",
    "        total_frames = video_info.total_frames\n",
    "        print(\"Start processing video: \", video_info)\n",
    "\n",
    "\n",
    "\n",
    "    def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "        detections = frame_inference(frame, threshold=threshold)\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "        detections = smoother.update_with_detections(detections)\n",
    "        annotated_frame = annotate_frame(frame, detections, threshold=threshold)\n",
    "        if verbose:\n",
    "            nonlocal frame_idx \n",
    "            frame_idx += 1\n",
    "            class_counts = {}\n",
    "            for detection in detections:\n",
    "                class_name = detection[-1]['class_name']\n",
    "                if class_name in class_counts:\n",
    "                    class_counts[class_name] += 1\n",
    "                else:\n",
    "                    class_counts[class_name] = 1\n",
    "            \n",
    "            detected_classes = {class_name: count for class_name, count in class_counts.items() if count > 0}\n",
    "            print(f\"Processing frame {frame_idx} of {total_frames}: size {frame.shape}, detections: {detected_classes}\")\n",
    "        return annotated_frame\n",
    "\n",
    "    sv.process_video(\n",
    "        source_path=video_path,\n",
    "        target_path=output_video_path,\n",
    "        callback=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path=PATH + \"/\" + VIDEO_NAME\n",
    "output_video_path=OUTPUT_PATH + \"/\" + OUTPUT_VIDEO_NAME\n",
    "\n",
    "sv.VideoInfo.from_video_path(video_path)\n",
    "\n",
    "inference_video(video_path, output_video_path, verbose=True, threshold=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
