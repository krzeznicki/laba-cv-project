{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR football training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q datasets transformers[torch] evaluate timm albumentations accelerate roboflow wandb torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y supervision && pip install -q supervision>=0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = \"facebook/detr-resnet-50-dc5\" # DETR\n",
    "#MODEL_NAME = \"jozhang97/deta-swin-large\" # Object detection leader on HF - based on paper https://arxiv.org/pdf/2212.06137\n",
    "MODEL_NAME = \"SenseTime/deformable-detr\" #Deformable DETR\n",
    "MODEL_CHECKPOINT = \"deformable-detr-football-finetuned\"\n",
    "PROJECT_NAME = \"football-detection\"\n",
    "RUN_NAME = \"eval test\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "#IMAGE_SIZE = 800 #not used rn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get API keys from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    rf_api_key = userdata.get(\"ROBOFLOW_API_KEY\")\n",
    "    wandb_api_key = userdata.get(\"WANDB_API_KEY\")\n",
    "    hf_token=userdata.get('HF_TOKEN')\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(dotenv_path='../config/.env')\n",
    "    rf_api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "    wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "dataset_version = 1\n",
    "dataset_location = \"../data/training\"\n",
    "\n",
    "\n",
    "\n",
    "rf = Roboflow(api_key=rf_api_key)\n",
    "project = rf.workspace(\"sport-cv\").project(\"football-players-detection-3zvbc-fynld\")\n",
    "version = project.version(2)\n",
    "robo_dataset = version.download(\"coco\", location=dataset_location)\n",
    "print(\"loaded dataset to\", dataset_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform dataset into a format that can be used by the DETR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_coco_dataset(json_file, dataset_base_path, split):\n",
    "    # Load JSON data\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    dataset_dict = {\n",
    "        'image_id': [],\n",
    "        'image': [],\n",
    "        'objects': [],\n",
    "        'width': [],\n",
    "        'height': []\n",
    "    }\n",
    "\n",
    "    for img in coco_data['images']:\n",
    "        img_id = img['id']\n",
    "        img_path = os.path.join(dataset_base_path, split, img['file_name'])\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        img_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == img_id]\n",
    "        \n",
    "        objects = {\n",
    "            'id': [],\n",
    "            'area': [],\n",
    "            'bbox': [],\n",
    "            'category': []\n",
    "        }\n",
    "        \n",
    "        for ann in img_annotations:\n",
    "            objects['id'].append(ann['category_id'])\n",
    "            objects['area'].append(ann['area'])\n",
    "            objects['bbox'].append(ann['bbox'])\n",
    "            # Find category name\n",
    "            category_name = next(\n",
    "                cat['name'] for cat in coco_data['categories'] \n",
    "                if cat['id'] == ann['category_id']\n",
    "            )\n",
    "            objects['category'].append(category_name)\n",
    "\n",
    "        # Add to dataset dictionary\n",
    "        dataset_dict['image_id'].append(img_id)\n",
    "        dataset_dict['image'].append(image)\n",
    "        dataset_dict['objects'].append(objects)\n",
    "        dataset_dict['width'].append(img['width'])\n",
    "        dataset_dict['height'].append(img['height'])\n",
    "\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "dataset_base_path = robo_dataset.location  \n",
    "train_json = os.path.join(dataset_base_path, \"train\", \"_annotations.coco.json\")\n",
    "val_json = os.path.join(dataset_base_path, \"valid\", \"_annotations.coco.json\")\n",
    "test_json = os.path.join(dataset_base_path, \"test\", \"_annotations.coco.json\")\n",
    "\n",
    "# Load datasets for each split\n",
    "train_dataset = load_coco_dataset(train_json, dataset_base_path, 'train')\n",
    "val_dataset = load_coco_dataset(val_json, dataset_base_path, 'valid')\n",
    "test_dataset = load_coco_dataset(test_json, dataset_base_path, 'test')\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"].select(range(48)) # validatation dataset len have to be multiplier of 8 because of a bug in collect_image_sizes\n",
    "test_dataset = dataset[\"test\"].select(range(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "def draw_image_from_idx(dataset, idx):\n",
    "    sample = dataset[idx]\n",
    "    image = sample[\"image\"]\n",
    "    annotations = sample[\"objects\"]\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = sample[\"width\"], sample[\"height\"]\n",
    "\n",
    "    for i in range(len(annotations[\"id\"])):\n",
    "        box = annotations[\"bbox\"][i]\n",
    "        class_idx = annotations[\"id\"][i]\n",
    "        x, y, w, h = tuple(box)\n",
    "        if max(box) > 1.0:\n",
    "            x1, y1 = int(x), int(y)\n",
    "            x2, y2 = int(x + w), int(y + h)\n",
    "        else:\n",
    "            x1 = int(x * width)\n",
    "            y1 = int(y * height)\n",
    "            x2 = int((x + w) * width)\n",
    "            y2 = int((y + h) * height)\n",
    "        draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=1)\n",
    "        draw.text((x1, y1), annotations[\"category\"][i], fill=\"white\")\n",
    "    return image\n",
    "\n",
    "\n",
    "import random\n",
    "random_idx = random.randint(0, len(train_dataset) - 1)\n",
    "draw_image_from_idx(dataset=train_dataset, idx=random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def plot_images(dataset, num_images=9):\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "    num_rows = num_images // 3\n",
    "    num_cols = 3\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Draw image\n",
    "        image = draw_image_from_idx(dataset, idx)\n",
    "\n",
    "        # Display image on the corresponding subplot\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Now use the function to plot images\n",
    "plot_images(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        #albumentations.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        albumentations.Perspective(p=0.1),\n",
    "        albumentations.RandomBrightnessContrast(p=0.5),\n",
    "        albumentations.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "        bbox_params=albumentations.BboxParams(\n",
    "        format=\"coco\",\n",
    "        label_fields=[\"category\"],\n",
    "        clip=True,\n",
    "        check_each_transform=True #TODO: sprawdzić działanie na False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we initialize all the transformations, we need to make a function which formats the annotations and returns the a list of annotation with a very specific format.\n",
    "\n",
    "This is because the image_processor expects the annotations to be in the following format: {'image_id': int, 'annotations': List[Dict]}, where each dictionary is a COCO object annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming a batch\n",
    "\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"id\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations for both train and test dataset\n",
    "\n",
    "train_dataset_transformed = train_dataset.with_transform(transform_aug_ann)\n",
    "validation_dataset_transformed = validation_dataset.with_transform(transform_aug_ann)\n",
    "test_dataset_transformed = test_dataset.with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collate_fn is responsible for taking a list of samples from a dataset and converting them into a batch suitable for model’s input format.\n",
    "\n",
    "In general a DataCollator typically performs tasks such as padding, truncating etc. In a custom collate function, we often define what and how we want to group the data into batches or simply, how to represent each batch.\n",
    "\n",
    "Data collator mainly puts the data together and then preprocesses them. Let’s make our collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "id2label = {1: \"ball\", 2: \"goalkeeper\", 3: \"player\", 4: \"referee\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "import torch\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # convert center to corners format\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # convert to absolute coordinates\n",
    "    height, width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import wandb\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import supervision as sv\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "class MAPEvaluator:\n",
    "\n",
    "    def __init__(self, image_processor, threshold=0.00, id2label=None):\n",
    "        self.image_processor = image_processor\n",
    "        self.threshold = threshold\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def collect_image_sizes(self, targets):\n",
    "        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n",
    "        image_sizes = []\n",
    "        for batch in targets:\n",
    "            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n",
    "            image_sizes.append(batch_image_sizes)\n",
    "        return image_sizes\n",
    "\n",
    "    def collect_targets(self, targets, image_sizes):\n",
    "        post_processed_targets = []\n",
    "        for target_batch, image_size_batch in zip(targets, image_sizes):\n",
    "            \n",
    "            for target, (height, width) in zip(target_batch, image_size_batch):\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes = sv.xcycwh_to_xyxy(boxes)\n",
    "                boxes = boxes * np.array([width, height, width, height])\n",
    "                boxes = torch.tensor(boxes)\n",
    "                labels = torch.tensor(target[\"class_labels\"])\n",
    "                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "        return post_processed_targets\n",
    "\n",
    "    def collect_predictions(self, predictions, image_sizes):\n",
    "        post_processed_predictions = []\n",
    "        for batch, target_sizes in zip(predictions, image_sizes):\n",
    "            batch_logits, batch_boxes = batch[1], batch[2]\n",
    "            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "            post_processed_output = self.image_processor.post_process_object_detection(\n",
    "                output, threshold=self.threshold, target_sizes=target_sizes\n",
    "            )\n",
    "            post_processed_predictions.extend(post_processed_output)\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results):\n",
    "\n",
    "        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "        image_sizes = self.collect_image_sizes(targets)\n",
    "        post_processed_targets = self.collect_targets(targets, image_sizes)\n",
    "        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "        evaluator.warn_on_many_detections = False\n",
    "        evaluator.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "        metrics = evaluator.compute()\n",
    "\n",
    "            # Prepare metrics for wandb\n",
    "        wandb_metrics = {}\n",
    "\n",
    "        wandb_metrics.update({\n",
    "          'mAP': metrics['map'].item(),\n",
    "          'mAP_50': metrics['map_50'].item(),\n",
    "          'mAP_75': metrics['map_75'].item(),\n",
    "          'mAR_1': metrics['mar_1'].item(),\n",
    "          'mAR_10': metrics['mar_10'].item(),\n",
    "          'mAR_100': metrics['mar_100'].item(),\n",
    "        })\n",
    "\n",
    "\n",
    "        # Replace list of per class metrics with separate metric for each class\n",
    "        classes = metrics.pop(\"classes\")\n",
    "        map_per_class = metrics.pop(\"map_per_class\")\n",
    "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "            if class_id == 0: #we don't want to use class with index 0\n",
    "                continue\n",
    "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "            metrics[f\"map_{class_name}\"] = class_map\n",
    "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "            wandb_metrics.update({\n",
    "            f'{class_name}/mAP': class_map.item(),\n",
    "            f'{class_name}/mAR': class_mar.item(),\n",
    "            })\n",
    "\n",
    "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "        wandb.log(wandb_metrics)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=image_processor, threshold=0.01, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "# Define the training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_CHECKPOINT,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    fp16=False,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub= True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_do_concat_batches=False\n",
    ")\n",
    "\n",
    "# Initialize wandb before training\n",
    "\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset_transformed,\n",
    "    eval_dataset=validation_dataset_transformed,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = trainer.evaluate(eval_dataset=test_dataset_transformed, metric_key_prefix=\"test\")\n",
    "pprint(metrics)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save the model to the hub, you need to change the model to your own username and the name of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "# make the object detection pipeline\n",
    "model_checkpoint = \"theButcher22/\" + MODEL_CHECKPOINT\n",
    "obj_detector = pipeline(\n",
    "    \"object-detection\", \n",
    "    model= model_checkpoint, \n",
    "    threshold=0.3\n",
    ")\n",
    "results = obj_detector(train_dataset[0][\"image\"])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont\n",
    "\n",
    "def plot_results(image, results, threshold=0.7):\n",
    "    image = Image.fromarray(np.uint8(image))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for result in results:\n",
    "        score = result[\"score\"]\n",
    "        label = result[\"label\"]\n",
    "        box = list(result[\"box\"].values())\n",
    "        if score > threshold:\n",
    "            x, y, x2, y2 = tuple(box)\n",
    "            draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "            draw.text(\n",
    "                (x + 0.5, y - 0.5),\n",
    "                text=str(label),\n",
    "                fill=\"green\" if score > 0.7 else \"red\",\n",
    "                font=ImageFont.load_default(size=16) # Increased font size\n",
    "            )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(image, results, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def predict(image, pipeline, threshold=0.1):\n",
    "    results = pipeline(image)\n",
    "    return plot_results(image, results, threshold)\n",
    "\n",
    "img = random.choice(test_dataset)[\"image\"]\n",
    "predict(img, obj_detector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
